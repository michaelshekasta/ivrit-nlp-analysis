{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import torch\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, Trainer, TrainingArguments\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport datasets\n\n# Load the pre-trained model and tokenizer\nmodel_name = \"xlm-roberta-large\"\nmodel = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n\n# Create a sample DataFrame\ndata = pd.DataFrame({\n    'text': [\n        'This is a positive example',\n        'This is a negative example',\n        'Another positive example',\n        'Another negative example'\n    ],\n    'label': [1, 0, 1, 0]\n})\n\n# Split the data into train and validation sets\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Define a function to preprocess the input text\ndef preprocess(examples):\n    texts = examples['text']\n    encodings = tokenizer(texts, padding=True, truncation=True, max_length=512)\n    labels = examples['label']\n    encodings['labels'] = labels\n    return encodings\n\n# Prepare the datasets\ntrain_dataset = datasets.Dataset.from_pandas(train_data)\ntrain_dataset = train_dataset.map(preprocess, batched=True)\nval_dataset = datasets.Dataset.from_pandas(val_data)\nval_dataset = val_dataset.map(preprocess, batched=True)\n\n# Set up the training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    report_to='none'\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Fine-tune the model\ntrainer.train()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:15:14.396665Z","iopub.execute_input":"2024-05-03T08:15:14.397090Z","iopub.status.idle":"2024-05-03T08:17:18.910733Z","shell.execute_reply.started":"2024-05-03T08:15:14.397057Z","shell.execute_reply":"2024-05-03T08:17:18.906740Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"053eb8640a00456580125a89c95a8d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"235b237ee58946b6ba9ae40010903bdd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 01:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.731687</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.833857</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.055301</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encodings\n\u001b[1;32m     71\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(preprocess_test, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m outputs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n\u001b[1;32m     75\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mpredictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'map'"],"ename":"AttributeError","evalue":"'dict' object has no attribute 'map'","output_type":"error"}]},{"cell_type":"code","source":"# Run inference on the test data\nimport torch\ntest_data = pd.DataFrame({'text': ['This is a test example', 'Another test example']})\n\ndef preprocess_test(examples):\n    texts = examples['text']\n    encodings = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n    return encodings\n\ntest_dataset = datasets.Dataset.from_pandas(test_data)\ntest_dataset = test_dataset.map(preprocess_test, batched=True)\n\noutputs = trainer.predict(test_dataset)\nprint(outputs.predictions)\npreds = torch.softmax(torch.tensor(outputs.predictions), dim=1)[:, 1].tolist()\n\n# Add the predictions to the DataFrame\ntest_data['probabilities'] = preds\n\n# Print the DataFrame\nprint(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T08:18:53.091103Z","iopub.execute_input":"2024-05-03T08:18:53.091710Z","iopub.status.idle":"2024-05-03T08:18:53.673199Z","shell.execute_reply.started":"2024-05-03T08:18:53.091664Z","shell.execute_reply":"2024-05-03T08:18:53.672281Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa872e4cca04b2dbcff94d5d05158f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"[[-0.12965813 -0.02505487]\n [-0.1799393   0.02218091]]\n                     text  probabilities\n0  This is a test example       0.526127\n1    Another test example       0.550359\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}